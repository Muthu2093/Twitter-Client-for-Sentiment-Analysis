Name=Sys.time()
Name=paste("Data Collected on",Sys.time(),sep = ";")
Name=paste("Data Collected on ",Sys.time(),)
Name=paste("Data Collected on ",Sys.time())
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file
Name=paste("Data Collected on ",Sys.time())
write.csv(tweets, file = Name)
install.packages("choroplethr")
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/heat_Map.R')
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/heat_Map.R')
activityLevel
states
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/heat_Map.R')
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/heat_Map.R')
debugSource('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
dev.off()
debugSource('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
rm(list =ls())
library(reshape)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Postive Tested/Workbook1.csv")
head(data)
tail(data)
data <- subset( data, select = -Total...Tested )
data <- data[-c(18, 19, 20, 21, 22), ]
data <- t(data)
colnames(data) <- data[1, ]
data <- data[-1, ]
label=c("W41","W41","W42","W43","W44","W45","W46","W47","W48","W49","W50","W51","W52","W01","W02","W03","W04")
colnames(data)= label
library(reshape)
data <- melt(data)
library(ggplot2)
label=c("Type.of.disease", "Week", "Total")
colnames(data) <- label
ggplot(data, aes(x=Week, y=Total, fill=Type.of.disease)) +
geom_bar(stat="identity") +
xlab("\n Weeks(2017: 41-52, 2018:01-04)") +
ylab("Total number of postive cases\n") +
theme_bw()
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
rm(list =ls())
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Postive Tested/Workbook1.csv")
head(data)
tail(data)
data <- subset( data, select = -Total...Tested )
data <- data[-c(18, 19, 20, 21, 22), ]
data <- t(data)
colnames(data) <- data[1, ]
data <- data[-1, ]
label=c("W41","W41","W42","W43","W44","W45","W46","W47","W48","W49","W50","W51","W52","W01","W02","W03","W04")
colnames(data)= label
data <- melt(data)
library(ggplot2)
label=c("Type.of.disease", "Week", "Total")
colnames(data) <- label
ggplot(data, aes(x=Week, y=Total, fill=Type.of.disease)) +
geom_bar(stat="identity") +
xlab("\n Weeks(2017: 41-52, 2018:01-04)") +
ylab("Total number of postive cases\n") +
theme_bw()
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R')
rm(list =ls())
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Postive Tested/Workbook1.csv")
head(data)
tail(data)
data <- subset( data, select = -Total...Tested )
data <- data[-c(18, 19, 20, 21, 22), ]
data <- t(data)
colnames(data) <- data[1, ]
data <- data[-1, ]
label=c("2017W41","2017W41","2017W42","2017W43","2017W44","2017W45","2017W46","2017W47","2017W48","2017W49","2017W50","2017W51","2017W52","2018W01","2018W02","2018W03","2018W04")
colnames(data)= label
data <- melt(data)
library(ggplot2)
label=c("Type.of.disease", "Week", "Total")
colnames(data) <- label
ggplot(data, aes(x=Week, y=Total, fill=Type.of.disease)) +
geom_bar(stat="identity") +
xlab("\n Weeks(2017: 41-52, 2018:01-04)") +
ylab("Total number of postive cases\n") +
theme_bw()
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R', echo=TRUE)
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R', echo=TRUE)
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/heat_Map.R', echo=TRUE)
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R', echo=TRUE)
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R', echo=TRUE)
activityLevel <- gsub("Level 10","red3", activityLevel)
rm(list =ls())
library(maps)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Heat Map/heatmap.csv")
states=data$STATENAME
activityLevel <- data$ACTIVITY.LEVEL
activityLevel <- gsub("Level 10","red3", activityLevel)
activityLevel <- gsub("Level 9","orangered1", activityLevel)
activityLevel <- gsub("Level 8","darkorange1", activityLevel)
activityLevel <- gsub("Level 7","darkgoldenrod2", activityLevel)
activityLevel <- gsub("Level 6","yellow3", activityLevel)
activityLevel <- gsub("Level 5","yellow1", activityLevel)
activityLevel <- gsub("Level 4","greenyellow", activityLevel)
activityLevel <- gsub("Level 3","green", activityLevel)
activityLevel <- gsub("Level 2","green3", activityLevel)
activityLevel <- gsub("Level 1","green4", activityLevel)
activityLevel <- gsub("Level 0","white", activityLevel)
#df <- melt(data.frame(data$STATENAME, data$ACTIVITY.LEVEL, activityLevel))
map(database = 'state',regions=data$STATENAME,  fill=TRUE, col=activityLevel,
resolution=0)
states
activityLevel
install.packages('fiftystater')
library(fiftystater)
map(database = 'fifty_states',regions=data$STATENAME,  fill=TRUE, col=activityLevel,
resolution=0)
library(fiftystater)
st=fifty_states
map(database = st,regions=data$STATENAME,  fill=TRUE, col=activityLevel,
resolution=0)
View(st)
#if you want to preserve the column order
#since the order may be informative
heatmap(data_matrix,Colv=NA,col=brewer.pal(9,"Blues"))
source('~/Desktop/Untitled.R', echo=TRUE)
rm(list =ls())
library(maps)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Heat Map/heatmap.csv")
states=data$STATENAME
activityLevel <- data$ACTIVITY.LEVEL
heatmap(activityLevel)
rm(list =ls())
library(maps)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Heat Map/heatmap.csv")
states=data$STATENAME
activityLevel <- data$ACTIVITY.LEVEL
activityLevel <- gsub("Level 10","10", activityLevel)
activityLevel <- gsub("Level 9","9", activityLevel)
activityLevel <- gsub("Level 8","8", activityLevel)
activityLevel <- gsub("Level 7","7", activityLevel)
activityLevel <- gsub("Level 6","6", activityLevel)
activityLevel <- gsub("Level 5","5", activityLevel)
activityLevel <- gsub("Level 4","4", activityLevel)
activityLevel <- gsub("Level 3","3", activityLevel)
activityLevel <- gsub("Level 2","2", activityLevel)
activityLevel <- gsub("Level 1","1", activityLevel)
activityLevel <- gsub("Level 0","0", activityLevel)
heatmap(activityLevel)
heatmap(as.nurmeric(activityLevel))
heatmap(data.frmae(activityLevel))
df_activityLevel=data.frame(activityLevel)
View(df_activityLevel)
df_activityLevel=data.frame(activityLevel)
heatmap((df_activityLevel))
heatmap(df_activityLevel(,1))
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R', echo=TRUE)
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/heat_Map.R', echo=TRUE)
rm(list =ls())
library(maps)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
data <- read.csv("../Data/Heat Map/heatmap.csv")
states=data$STATENAME
activityLevel <- data$ACTIVITY.LEVEL
activityLevel <- gsub("Level 10","10", activityLevel)
activityLevel <- gsub("Level 9","9", activityLevel)
activityLevel <- gsub("Level 8","8", activityLevel)
activityLevel <- gsub("Level 7","7", activityLevel)
activityLevel <- gsub("Level 6","6", activityLevel)
activityLevel <- gsub("Level 5","5", activityLevel)
activityLevel <- gsub("Level 4","4", activityLevel)
activityLevel <- gsub("Level 3","3", activityLevel)
activityLevel <- gsub("Level 2","2", activityLevel)
activityLevel <- gsub("Level 1","1", activityLevel)
activityLevel <- gsub("Level 0","0", activityLevel)
df_activityLevel=data.frame(activityLevel)
#heatmap(df_activityLevel(,1))
#df <- melt(data.frame(data$STATENAME, data$ACTIVITY.LEVEL, activityLevel))
library(fiftystater)
map(database = 'state',regions=data$STATENAME,  fill=TRUE, col=activityLevel,
resolution=0)
heatmap colors()
fiftystates
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts")
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part2/Scripts/postive_Tested.R', echo=TRUE)
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#flu"
no.of.tweets <- 1500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
Name=paste("New", no.of.tweets," Tweets Collected on ",Sys.time())
write.csv(tweets, file = Name)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
# Reading all tweets collected so far
CDF=read.csv("../data_collected/consolidated_Tweets_Total")
CDF<- subset(CDF, select = -c(X)) #removing column named X
# Creating a consolided data frame of all the tweets collected so far
consolidated_Tweets_Total=rbind(CDF,tweets)
consolidated_Tweets_Total <- unique( consolidated_Tweets_Total[ , 1:16 ] ) #remove duplicates
# Saving all Tweets Collected from day 1 to csv file
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(consolidated_Tweets_Total, file = "consolidated_Tweets_Total")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
################# Filtering Tweets ####################
## Eliminating duplicate users by lookingUp screenName
usernames <- tweets$screenName
temp_df <- twListToDF(lookupUsers(usernames))
## Remove users without any location information
DWL=read.csv("../data_collected/data_With_location")
DWL<- subset(DWL, select = -c(X)) #removing column named X
tweets_With_location <- subset(temp_df, temp_df$location != "")
data_With_location <- rbind(DWL, tweets_With_location)
data_With_location <- unique( data_With_location[ , 1:17 ] )
#Saving the data with location to csv file
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(data_With_location, file = "data_With_location")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
########################## Fetching Geocode of users #######################
## Code to access geocode - limit 2500 per day - dont waste it
## Use it after you have extracted tweets with location info (20 to 30 maybe)
# locatedUsers <- !is.na(tweets_with_location$location)
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#flu"
no.of.tweets <- 1300
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
Name=paste("New", no.of.tweets," Tweets Collected on ",Sys.time())
write.csv(tweets, file = Name)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
# Reading all tweets collected so far
CDF=read.csv("../data_collected/consolidated_Tweets_Total")
CDF<- subset(CDF, select = -c(X)) #removing column named X
# Creating a consolided data frame of all the tweets collected so far
consolidated_Tweets_Total=rbind(CDF,tweets)
consolidated_Tweets_Total <- unique( consolidated_Tweets_Total[ , 1:16 ] ) #remove duplicates
# Saving all Tweets Collected from day 1 to csv file
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(consolidated_Tweets_Total, file = "consolidated_Tweets_Total")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
################# Filtering Tweets ####################
## Eliminating duplicate users by lookingUp screenName
usernames <- tweets$screenName
temp_df <- twListToDF(lookupUsers(usernames))
## Remove users without any location information
DWL=read.csv("../data_collected/data_With_location")
DWL<- subset(DWL, select = -c(X)) #removing column named X
tweets_With_location <- subset(temp_df, temp_df$location != "")
data_With_location <- rbind(DWL, tweets_With_location)
data_With_location <- unique( data_With_location[ , 1:17 ] )
#Saving the data with location to csv file
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(data_With_location, file = "data_With_location")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
########################## Fetching Geocode of users #######################
## Code to access geocode - limit 2500 per day - dont waste it
## Use it after you have extracted tweets with location info (20 to 30 maybe)
# locatedUsers <- !is.na(tweets_with_location$location)
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
View(locations)
View(location_GeoCode)
tp <- subset(location_GeoCode, location_GeoCode$lon != "")
View(tp)
tp <- subset(location_GeoCode, location_GeoCode$lon != "NA")
tp <- subset(location_GeoCode, location_GeoCode$lon != NA)
tp <- subset(location_GeoCode, location_GeoCode$lon != "NA")
tp <- subset(location_GeoCode, location_GeoCode$lon != "")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#flu"
no.of.tweets <- 1300
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
Name=paste("New", no.of.tweets," Tweets Collected on ",Sys.time())
write.csv(tweets, file = Name)
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
# Reading all tweets collected so far
CDF=read.csv("../data_collected/consolidated_Tweets_Total")
CDF<- subset(CDF, select = -c(X)) #removing column named X
# Creating a consolided data frame of all the tweets collected so far
consolidated_Tweets_Total=rbind(CDF,tweets)
consolidated_Tweets_Total <- unique( consolidated_Tweets_Total[ , 1:16 ] ) #remove duplicates
# Saving all Tweets Collected from day 1 to csv file
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(consolidated_Tweets_Total, file = "consolidated_Tweets_Total")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
################# Filtering Tweets ####################
## Eliminating duplicate users by lookingUp screenName
usernames <- tweets$screenName
temp_df <- twListToDF(lookupUsers(usernames))
## Remove users without any location information
DWL=read.csv("../data_collected/data_With_location")
DWL<- subset(DWL, select = -c(X)) #removing column named X
tweets_With_location <- subset(temp_df, temp_df$location != "")
data_With_location <- rbind(DWL, tweets_With_location)
data_With_location <- unique( data_With_location[ , 1:17 ] )
#Saving the data with location to csv file
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(data_With_location, file = "data_With_location")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
########################## Fetching Geocode of users #######################
## Code to access geocode - limit 2500 per day - dont waste it
## Use it after you have extracted tweets with location info (20 to 30 maybe)
# locatedUsers <- !is.na(tweets_with_location$location)
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
location_GeoCode <- subset(location_GeoCode, location_GeoCode$lon != "")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/data_collected")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("/Users/muthuvel/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
location_GeoCode <- subset(location_GeoCode, location_GeoCode$lon != "")
setwd("../data_collected")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("../Scripts")
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
location_GeoCode <- subset(location_GeoCode, location_GeoCode$lon != "")
setwd("../data_collected")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("../Scripts")
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
search.string <- "#flu"
no.of.tweets <- 1300
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets <- twListToDF(tweets)
setwd("../data_collected")
Name=paste("New", no.of.tweets," Tweets Collected on ",Sys.time())
write.csv(tweets, file = Name)
setwd("../Scripts")
CDF=read.csv("../data_collected/consolidated_Tweets_Total")
CDF<- subset(CDF, select = -c(X)) #removing column named X
consolidated_Tweets_Total=rbind(CDF,tweets)
consolidated_Tweets_Total <- unique( consolidated_Tweets_Total[ , 1:16 ] ) #remove duplicates
setwd("../data_collected")
write.csv(consolidated_Tweets_Total, file = "consolidated_Tweets_Total")
setwd("../Scripts")
usernames <- tweets$screenName
temp_df <- twListToDF(lookupUsers(usernames))
DWL=read.csv("../data_collected/data_With_location")
DWL<- subset(DWL, select = -c(X)) #removing column named X
tweets_With_location <- subset(temp_df, temp_df$location != "")
data_With_location <- rbind(DWL, tweets_With_location)
data_With_location <- unique( data_With_location[ , 1:17 ] )
setwd("../data_collected")
write.csv(data_With_location, file = "data_With_location")
setwd("../Scripts")
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
