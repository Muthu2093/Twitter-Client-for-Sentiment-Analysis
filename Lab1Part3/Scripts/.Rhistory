q()
q()
q()
q()
defaults write org.R-project.R force.LANG en_US.UTF-8
q()
install.packages("ggplot")
install.packages("ggplot2")
jupyter notebook
q()
install.packages("sp")
install.packages('gglplot2', dependencies = TRUE)
install.packages('ggplot2', dependencies = TRUE)
warnings()
q()
remove.packages("ggplot2")
install.packages('ggplot2', dependencies = TRUE)
q()
install.packages("sp")
conda install clangxx_osx-64
conda install clangxx_osx-64
q()
install.packages("twitteR")
install.packages(c("devtools", "rjson", "bit64", "httr"))
q()
library(twitteR)
install.packages("twitteR")
q(0
q()
q()
install.packages("twitteR", dependencies = TRUE)
library(twitteR)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
install.packages("httpuv")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
install.packages("httpuv", dependencies=TRUE)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
devtools::install_github("rstudio/httpuv")
install_github("rstudio/httpuv")
getTwitterOAuth(consumer_key, consumer_secret)
getTwitterOAuth(VxJ6qp5XL3VTclBzMBsD1Ez1A, owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m)
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
remove.packages("httpuv")
install.packages("httpuv")
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m")
cred <- OAuthFactory$new(consumerKey='VxJ6qp5XL3VTclBzMBsD1Ez1A',
consumerSecret='owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
tweets
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets,lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 10000
tweets <- searchTwitter(search.string, n=no.of.tweets, geocode="40.7128,74.0060,150mi", lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
tweets
search.string <- "#flu"
no.of.tweets <- 10000
tweets <- searchTwitter(search.string, n=no.of.tweets, geocode="40.7128,74.0060,150mi", lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
View(tweets)
search.string <- "#flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
tweets
View(tweets)
View(tweets)
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 15000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
search.string <- "flu"
no.of.tweets <- 5000
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
Name=Sys.Date()+Sys.time()
Name=paste(Sys.Date(),Sys.time(),sep = ";")
Name=Sys.time()
Name=paste("Data Collected on",Sys.time(),sep = ";")
Name=paste("Data Collected on ",Sys.time(),)
Name=paste("Data Collected on ",Sys.time())
rm(list =ls())
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
## Searching for tweets ##
search.string <- "flu"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file
Name=paste("Data Collected on ",Sys.time())
write.csv(tweets, file = Name)
install.packages("choroplethr")
setwd("../data_collected")
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
rm(list =ls())
#### SET THE WORKING DIRECTORY TO 'SCRIPTS' FOLDER IN LAB1 PART3 BEFORE RUNNING THE SCRIPT####
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
setwd("~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts")
setwd("../data_collected")
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
View(location_GeoCode)
View(location_GeoCode)
value=location_GeoCode
a <- location_GeoCode[15299:15826,:]
a <- subset(location_GeoCode, location_GeoCode[15299:15826])
a <- location_GeoCode(r(15299:15826))
a <- location_GeoCode[r(15299:15826)]
a <- location_GeoCode[c(15299:15826)]
t(value)
b= t(value)
View(b)
a <- location_GeoCode[c(15299:15826)]
a <- location_GeoCode[c(15299:15825)]
a <- value[c(15299:15825)]
a <- b[c(15299:15825)]
a= t(a)
View(a)
View(b)
a <- b[c(15299:15825)]
a <- b[:,c(15299:15825)]
View(b)
View(b)
View(location_GeoCode)
setwd("../Scripts")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
write.csv(location_GeoCode, file = "location_GeoCode")
rm(list =ls())
#### SET THE WORKING DIRECTORY TO 'SCRIPTS' FOLDER IN LAB1 PART3 BEFORE RUNNING THE SCRIPT####
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#flu"
no.of.tweets <- 1300
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("../data_collected")
Name=paste("New", no.of.tweets," Tweets Collected on ",Sys.time())
write.csv(tweets, file = Name)
setwd("../Scripts")
# Reading all tweets collected so far
CDF=read.csv("../data_collected/consolidated_Tweets_Total")
CDF<- subset(CDF, select = -c(X)) #removing column named X
# Creating a consolided data frame of all the tweets collected so far
consolidated_Tweets_Total=rbind(CDF,tweets)
consolidated_Tweets_Total <- unique( consolidated_Tweets_Total[ , 1:16 ] ) #remove duplicates
# Saving all Tweets Collected from day 1 to csv file
setwd("../data_collected")
write.csv(consolidated_Tweets_Total, file = "consolidated_Tweets_Total")
setwd("../Scripts")
################# Filtering Tweets ####################
## Eliminating duplicate users by lookingUp screenName
usernames <- tweets$screenName
temp_df <- twListToDF(lookupUsers(usernames))
## Remove users without any location information
DWL=read.csv("../data_collected/data_With_location")
DWL<- subset(DWL, select = -c(X)) #removing column named X
tweets_With_location <- subset(temp_df, temp_df$location != "")
data_With_location <- rbind(DWL, tweets_With_location)
data_With_location <- unique( data_With_location[ , 1:17 ] )
#Saving the data with location to csv file
setwd("../data_collected")
write.csv(data_With_location, file = "data_With_location")
setwd("../Scripts")
########################## Fetching Geocode of users #######################
## Code to access geocode - limit 2500 per day - dont waste it
## Use it after you have extracted tweets with location info (20 to 30 maybe)
# locatedUsers <- !is.na(tweets_with_location$location)
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
setwd("../data_collected")
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
location_GeoCode <- subset(location_GeoCode, location_GeoCode$lon != "")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("../Scripts")
rm(list =ls())
#### SET THE WORKING DIRECTORY TO 'SCRIPTS' FOLDER IN LAB1 PART3 BEFORE RUNNING THE SCRIPT####
## loading libraries
library(twitteR)
library(ggplot2)
library(ggmap)
library(data.table)
## Setup oauth
setup_twitter_oauth("VxJ6qp5XL3VTclBzMBsD1Ez1A", "owezT5IVRVG8nvkSHXxqq4t2McwPO6mxesJTGU2549yHTJbP8m", "340449785-0AWt3nkBVvLlX7hbUFLl0fEqIKs47qUU7V5UnFWH", "qnaD0Pyp9jUXfwVb82RlSKikuvVi2MAWxp1J0mD1Fle4d")
############## Collection of Tweets ###################
## Searching for tweets ##
search.string <- "#flu"
no.of.tweets <- 3500
tweets <- searchTwitter(search.string, n=no.of.tweets, lang="en")
## Conversion of searched tweets to Data frame
tweets <- twListToDF(tweets)
## Saving collected data to a csv file - only the tweets collection this session
setwd("../data_collected")
Name=paste("New", no.of.tweets," Tweets Collected on ",Sys.time())
write.csv(tweets, file = Name)
setwd("../Scripts")
# Reading all tweets collected so far
CDF=read.csv("../data_collected/consolidated_Tweets_Total")
CDF<- subset(CDF, select = -c(X)) #removing column named X
# Creating a consolided data frame of all the tweets collected so far
consolidated_Tweets_Total=rbind(CDF,tweets)
consolidated_Tweets_Total <- unique( consolidated_Tweets_Total[ , 1:16 ] ) #remove duplicates
# Saving all Tweets Collected from day 1 to csv file
setwd("../data_collected")
write.csv(consolidated_Tweets_Total, file = "consolidated_Tweets_Total")
setwd("../Scripts")
################# Filtering Tweets ####################
## Eliminating duplicate users by lookingUp screenName
usernames <- tweets$screenName
temp_df <- twListToDF(lookupUsers(usernames))
## Remove users without any location information
DWL=read.csv("../data_collected/data_With_location")
DWL<- subset(DWL, select = -c(X)) #removing column named X
tweets_With_location <- subset(temp_df, temp_df$location != "")
data_With_location <- rbind(DWL, tweets_With_location)
data_With_location <- unique( data_With_location[ , 1:17 ] )
#Saving the data with location to csv file
setwd("../data_collected")
write.csv(data_With_location, file = "data_With_location")
setwd("../Scripts")
########################## Fetching Geocode of users #######################
## Code to access geocode - limit 2500 per day - dont waste it
## Use it after you have extracted tweets with location info (20 to 30 maybe)
# locatedUsers <- !is.na(tweets_with_location$location)
j<-1;
for (i in tweets_With_location$location){
loc <- i
if (stringi::stri_enc_mark(loc)=="ASCII"){
if (j==1){
locations <- geocode(loc)
}
if (j>1){
locations <- rbind(locations,geocode(loc))
}
j <- j+1
}
}
setwd("../data_collected")
location_GeoCode <- read.csv("../data_collected/location_GeoCode")
location_GeoCode<- subset(location_GeoCode, select = -c(X)) #removing column named X
location_GeoCode <- rbind(location_GeoCode,locations)
location_GeoCode <- subset(location_GeoCode, location_GeoCode$lon != "")
write.csv(location_GeoCode, file = "location_GeoCode")
setwd("../Scripts")
source('~/Documents/GitHub/Twitter-client-for-Data-Collection-and-Exploratory-Data-Analysis-/Lab1Part3/Scripts/tweetCollector_Script.R')
